{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9051a4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sqlalchemy pymysql pandas scikit-learn imbalanced-learn joblib fastapi uvicorn pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1a32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas sqlalchemy pymysql scikit-learn imbalanced-learn xgboost joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac6cfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Shape: (20, 17)\n",
      "Sample data:\n",
      "   student_id     name  age  gender  academic_performance  stress_level  \\\n",
      "0           1    Alice   20  Female                     4             6   \n",
      "1           2      Bob   22    Male                     3             8   \n",
      "2           3  Charlie   23    Male                     5             4   \n",
      "3           4    David   21    Male                     2             9   \n",
      "4           5      Eva   24  Female                     5             3   \n",
      "\n",
      "   sleep_quality  anxiety_level  exercise_hours  study_hours  social_activity  \\\n",
      "0              3              7             2.5          5.0                2   \n",
      "1              2              9             1.0          6.0                1   \n",
      "2              4              5             4.0          7.0                3   \n",
      "3              2              8             0.5          4.5                0   \n",
      "4              4              4             5.0          7.5                4   \n",
      "\n",
      "   financial_stress part_time_job  screen_time  diet_quality  smoking  alcohol  \n",
      "0                 6            No          6.5             7        0      1.2  \n",
      "1                 9           Yes          8.0             5        3      4.5  \n",
      "2                 5            No          5.0             8        0      0.5  \n",
      "3                10           Yes          9.5             4        5      6.0  \n",
      "4                 4            No          4.5             9        0      0.0  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   student_id            20 non-null     int64  \n",
      " 1   name                  20 non-null     object \n",
      " 2   age                   20 non-null     int64  \n",
      " 3   gender                20 non-null     object \n",
      " 4   academic_performance  20 non-null     int64  \n",
      " 5   stress_level          20 non-null     int64  \n",
      " 6   sleep_quality         20 non-null     int64  \n",
      " 7   anxiety_level         20 non-null     int64  \n",
      " 8   exercise_hours        20 non-null     float64\n",
      " 9   study_hours           20 non-null     float64\n",
      " 10  social_activity       20 non-null     int64  \n",
      " 11  financial_stress      20 non-null     int64  \n",
      " 12  part_time_job         20 non-null     object \n",
      " 13  screen_time           20 non-null     float64\n",
      " 14  diet_quality          20 non-null     int64  \n",
      " 15  smoking               20 non-null     int64  \n",
      " 16  alcohol               20 non-null     float64\n",
      "dtypes: float64(4), int64(10), object(3)\n",
      "memory usage: 2.8+ KB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      "student_id              0\n",
      "name                    0\n",
      "age                     0\n",
      "gender                  0\n",
      "academic_performance    0\n",
      "stress_level            0\n",
      "sleep_quality           0\n",
      "anxiety_level           0\n",
      "exercise_hours          0\n",
      "study_hours             0\n",
      "social_activity         0\n",
      "financial_stress        0\n",
      "part_time_job           0\n",
      "screen_time             0\n",
      "diet_quality            0\n",
      "smoking                 0\n",
      "alcohol                 0\n",
      "dtype: int64\n",
      "\n",
      "Data after dropping columns: (20, 15)\n",
      "\n",
      "Class distribution:\n",
      "stress_level\n",
      "6    4\n",
      "8    3\n",
      "4    3\n",
      "9    3\n",
      "3    3\n",
      "7    3\n",
      "5    1\n",
      "Name: count, dtype: int64\n",
      "Too many stress levels detected. Binning into 3 categories...\n",
      "New class distribution:\n",
      "stress_level\n",
      "0    7\n",
      "1    7\n",
      "2    6\n",
      "Name: count, dtype: int64\n",
      "Categorical columns: ['gender', 'part_time_job']\n",
      "Numerical columns: ['age', 'academic_performance', 'sleep_quality', 'anxiety_level', 'exercise_hours', 'study_hours', 'social_activity', 'financial_stress', 'screen_time', 'diet_quality', 'smoking', 'alcohol']\n",
      "\n",
      "Training set shape: (16, 14)\n",
      "Testing set shape: (4, 14)\n",
      "Training set class distribution:\n",
      "stress_level\n",
      "1    6\n",
      "0    5\n",
      "2    5\n",
      "Name: count, dtype: int64\n",
      "Test set class distribution:\n",
      "stress_level\n",
      "0    2\n",
      "1    1\n",
      "2    1\n",
      "Name: count, dtype: int64\n",
      "Minimum class size in training data: 5\n",
      "\n",
      "==================================================\n",
      "MODEL EVALUATION WITH CROSS-VALIDATION\n",
      "==================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\n",
      "Logistic Regression Results:\n",
      "CV Accuracy: 0.9333 (±0.1333)\n",
      "Test Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
      "\n",
      "Training Random Forest...\n",
      "Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\n",
      "Random Forest Results:\n",
      "CV Accuracy: 0.8667 (±0.1633)\n",
      "Test Accuracy: 0.7500, Precision: 0.5833, Recall: 0.7500, F1-score: 0.6500\n",
      "\n",
      "Training SVM...\n",
      "Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\n",
      "SVM Results:\n",
      "CV Accuracy: 0.8667 (±0.1633)\n",
      "Test Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
      "\n",
      "Training Gradient Boosting...\n",
      "Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\n",
      "Gradient Boosting Results:\n",
      "CV Accuracy: 0.8000 (±0.1633)\n",
      "Test Accuracy: 0.7500, Precision: 0.5833, Recall: 0.7500, F1-score: 0.6500\n",
      "\n",
      "Training XGBoost...\n",
      "Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\n",
      "XGBoost Results:\n",
      "CV Accuracy: 0.7500 (±0.1291)\n",
      "Test Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1-score: 1.0000\n",
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "Best model: Logistic Regression with accuracy: 1.0000\n",
      "\n",
      "Performing hyperparameter tuning for Logistic Regression...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "Best parameters for Logistic Regression: {'model__C': 0.01, 'model__solver': 'lbfgs'}\n",
      "Best cross-validation score: 0.8667\n",
      "\n",
      "Final model test accuracy: 0.7500\n",
      "Final model test precision: 0.6250\n",
      "Final model test recall: 0.7500\n",
      "Final model test F1-score: 0.6667\n",
      "\n",
      "Classification Report for Final Model:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.75         4\n",
      "   macro avg       0.50      0.67      0.56         4\n",
      "weighted avg       0.62      0.75      0.67         4\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]]\n",
      "\n",
      "✅ Model pipeline and preprocessor saved successfully!\n",
      "Loaded pipeline accuracy: 0.7500\n",
      "✅ Pipeline loaded and tested successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Database connection parameters\n",
    "DB_USER = \"root\"\n",
    "DB_PASS = \"050901\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_NAME = \"test_db\"\n",
    "\n",
    "try:\n",
    "    # Create database connection\n",
    "    engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASS}@{DB_HOST}/{DB_NAME}\")\n",
    "    \n",
    "    # Fetch data from students table\n",
    "    query = \"SELECT * FROM students\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(\"Data loaded successfully. Shape:\", df.shape)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to database: {e}\")\n",
    "    # Create sample data for demonstration if database connection fails\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'student_id': range(1, n_samples+1),\n",
    "        'name': [f'Student_{i}' for i in range(1, n_samples+1)],\n",
    "        'age': np.random.randint(18, 25, n_samples),\n",
    "        'gender': np.random.choice(['Male', 'Female'], n_samples),\n",
    "        'major': np.random.choice(['CS', 'Engineering', 'Business', 'Arts'], n_samples),\n",
    "        'gpa': np.random.uniform(2.0, 4.0, n_samples),\n",
    "        'study_hours': np.random.randint(5, 40, n_samples),\n",
    "        'sleep_hours': np.random.randint(4, 10, n_samples),\n",
    "        'extracurricular': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'stress_level': np.random.choice([0, 1, 2], n_samples, p=[0.7, 0.2, 0.1])\n",
    "    })\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(df.info())\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['student_id', 'name']\n",
    "df = df.drop(columns=[col for col in columns_to_drop if col in df.columns], errors='ignore')\n",
    "print(f\"\\nData after dropping columns: {df.shape}\")\n",
    "\n",
    "# Check if stress_level column exists\n",
    "if 'stress_level' not in df.columns:\n",
    "    print(\"Error: 'stress_level' column not found in the dataset!\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "    # Create a sample stress_level column for demonstration\n",
    "    df['stress_level'] = np.random.choice([0, 1, 2], len(df), p=[0.7, 0.2, 0.1])\n",
    "    print(\"Created sample 'stress_level' column for demonstration\")\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "class_distribution = df['stress_level'].value_counts()\n",
    "print(class_distribution)\n",
    "\n",
    "# If there are too many classes or some classes have very few samples, we might need to bin them\n",
    "if len(df['stress_level'].unique()) > 5:\n",
    "    print(\"Too many stress levels detected. Binning into 3 categories...\")\n",
    "    # Bin into 3 categories: Low (0), Medium (1), High (2)\n",
    "    if df['stress_level'].max() > 2:\n",
    "        df['stress_level'] = pd.cut(df['stress_level'], bins=3, labels=[0, 1, 2])\n",
    "    print(\"New class distribution:\")\n",
    "    print(df['stress_level'].value_counts())\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('stress_level', axis=1)\n",
    "y = df['stress_level']\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical data\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "print(\"Training set class distribution:\")\n",
    "train_class_dist = pd.Series(y_train).value_counts()\n",
    "print(train_class_dist)\n",
    "print(\"Test set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "\n",
    "# Check if any class has too few samples for SMOTE\n",
    "min_class_size = train_class_dist.min()\n",
    "print(f\"Minimum class size in training data: {min_class_size}\")\n",
    "\n",
    "# Define models with regularization to prevent overfitting\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000, C=0.1),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100, max_depth=5),\n",
    "    'SVM': SVC(random_state=42, C=1.0, kernel='rbf', probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100, max_depth=3, learning_rate=0.1),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, n_estimators=100, max_depth=3, learning_rate=0.1, \n",
    "                                reg_alpha=0.1, reg_lambda=1.0, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Define parameter grids for hyperparameter tuning\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'model__C': [0.01, 0.1, 1, 10],\n",
    "        'model__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__max_depth': [3, 5, 7, None],\n",
    "        'model__min_samples_split': [2, 5, 10],\n",
    "        'model__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model__C': [0.1, 1, 10],\n",
    "        'model__kernel': ['linear', 'rbf'],\n",
    "        'model__gamma': ['scale', 'auto']\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "        'model__subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model__n_estimators': [50, 100, 200],\n",
    "        'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'model__max_depth': [3, 4, 5],\n",
    "        'model__reg_alpha': [0, 0.1, 0.5],\n",
    "        'model__reg_lambda': [0.5, 1.0, 1.5],\n",
    "        'model__subsample': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_pipeline = None\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL EVALUATION WITH CROSS-VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use stratified k-fold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    try:\n",
    "        # Adjust SMOTE parameters based on class sizes\n",
    "        # If minimum class size is too small, reduce n_neighbors or don't use SMOTE\n",
    "        if min_class_size <= 5:\n",
    "            print(\"Small class sizes detected. Using RandomUnderSampler instead of SMOTE.\")\n",
    "            # Create pipeline with preprocessing and model (without SMOTE)\n",
    "            pipeline = ImbPipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('sampling', RandomUnderSampler(random_state=42)),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        else:\n",
    "            # Use SMOTE with adjusted n_neighbors\n",
    "            n_neighbors = min(5, min_class_size - 1)  # Ensure n_neighbors <= min_class_size\n",
    "            print(f\"Using SMOTE with n_neighbors={n_neighbors}\")\n",
    "            pipeline = ImbPipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('sampling', SMOTE(random_state=42, sampling_strategy='not majority', k_neighbors=n_neighbors)),\n",
    "                ('model', model)\n",
    "            ])\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "        \n",
    "        # Fit the pipeline\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} Results:\")\n",
    "        print(f\"CV Accuracy: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = model\n",
    "            best_model_name = name\n",
    "            best_pipeline = pipeline\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error training {name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if best_model is None:\n",
    "    print(\"No suitable model found. Trying simple pipeline without sampling...\")\n",
    "    # Try without any sampling\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            print(f\"Trying {name} without sampling...\")\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            \n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            y_pred = pipeline.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model\n",
    "                best_model_name = name\n",
    "                best_pipeline = pipeline\n",
    "                \n",
    "            print(f\"{name} without sampling - Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name} without sampling: {e}\")\n",
    "\n",
    "if best_model is None:\n",
    "    print(\"All models failed. Using simple Logistic Regression as fallback.\")\n",
    "    best_model_name = \"Logistic Regression\"\n",
    "    best_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "    best_pipeline.fit(X_train, y_train)\n",
    "    y_pred = best_pipeline.predict(X_test)\n",
    "    best_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best model: {best_model_name} with accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Hyperparameter tuning for the best model (if we have a parameter grid for it)\n",
    "if best_model_name in param_grids:\n",
    "    print(f\"\\nPerforming hyperparameter tuning for {best_model_name}...\")\n",
    "\n",
    "    # Create a new pipeline for tuning (without sampling to avoid issues)\n",
    "    tuning_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', models[best_model_name])\n",
    "    ])\n",
    "\n",
    "    # Get the parameter grid for the best model\n",
    "    param_grid = param_grids[best_model_name]\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(\n",
    "        tuning_pipeline, \n",
    "        param_grid, \n",
    "        cv=cv, \n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"Best parameters for {best_model_name}: {best_params}\")\n",
    "    print(f\"Best cross-validation score: {best_score:.4f}\")\n",
    "\n",
    "    # Train final model with best parameters\n",
    "    final_pipeline = grid_search.best_estimator_\n",
    "else:\n",
    "    # If no hyperparameter tuning is needed, use the best pipeline\n",
    "    final_pipeline = best_pipeline\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate final model\n",
    "y_pred_final = final_pipeline.predict(X_test)\n",
    "final_accuracy = accuracy_score(y_test, y_pred_final)\n",
    "final_precision = precision_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
    "final_recall = recall_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
    "final_f1 = f1_score(y_test, y_pred_final, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nFinal model test accuracy: {final_accuracy:.4f}\")\n",
    "print(f\"Final model test precision: {final_precision:.4f}\")\n",
    "print(f\"Final model test recall: {final_recall:.4f}\")\n",
    "print(f\"Final model test F1-score: {final_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report for Final Model:\")\n",
    "print(classification_report(y_test, y_pred_final, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_final))\n",
    "\n",
    "# Save the final pipeline\n",
    "joblib.dump(final_pipeline, 'stress_level_pipeline.pkl')\n",
    "\n",
    "# Save the preprocessor separately as well\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "print(\"\\n✅ Model pipeline and preprocessor saved successfully!\")\n",
    "\n",
    "# Test the saved pipeline\n",
    "loaded_pipeline = joblib.load('stress_level_pipeline.pkl')\n",
    "y_pred_loaded = loaded_pipeline.predict(X_test)\n",
    "loaded_accuracy = accuracy_score(y_test, y_pred_loaded)\n",
    "\n",
    "print(f\"Loaded pipeline accuracy: {loaded_accuracy:.4f}\")\n",
    "print(\"✅ Pipeline loaded and tested successfully!\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if hasattr(final_pipeline.named_steps['model'], 'feature_importances_'):\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    feature_importances = final_pipeline.named_steps['model'].feature_importances_\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    try:\n",
    "        # For numerical features\n",
    "        num_features = numerical_cols\n",
    "        \n",
    "        # For categorical features\n",
    "        cat_transformer = preprocessor.named_transformers_['cat']\n",
    "        cat_features = cat_transformer.named_steps['onehot'].get_feature_names_out(categorical_cols)\n",
    "        \n",
    "        # Combine all feature names\n",
    "        all_features = list(num_features) + list(cat_features)\n",
    "        \n",
    "        # Create a DataFrame of feature importances\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': all_features,\n",
    "            'importance': feature_importances\n",
    "        })\n",
    "        \n",
    "        # Sort by importance\n",
    "        importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(importance_df.head(10))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not extract feature names: {e}\")\n",
    "        print(\"Top 10 feature importances (values):\")\n",
    "        print(sorted(feature_importances, reverse=True)[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
